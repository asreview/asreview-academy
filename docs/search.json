[
  {
    "objectID": "ASReviewLAB.html",
    "href": "ASReviewLAB.html",
    "title": "Introductory exercise to AI-aided screening using ASReview LAB",
    "section": "",
    "text": "The goal of this exercise is to get familiar with AI-aided screening by making use of ASReview LAB v1.x.\nYou will learn how to install and set up the software, upload data, screen records, and export and interpret the results. The exercise will guide you through all the steps of AI-aided screening as described in the workflow on Read-the-Docs.\nEnjoy!"
  },
  {
    "objectID": "ASReviewLAB.html#introduction-to-the-software-asreview-lab",
    "href": "ASReviewLAB.html#introduction-to-the-software-asreview-lab",
    "title": "Introductory exercise to AI-aided screening using ASReview LAB",
    "section": "",
    "text": "The goal of this exercise is to get familiar with AI-aided screening by making use of ASReview LAB v1.x.\nYou will learn how to install and set up the software, upload data, screen records, and export and interpret the results. The exercise will guide you through all the steps of AI-aided screening as described in the workflow on Read-the-Docs.\nEnjoy!"
  },
  {
    "objectID": "ASReviewLAB.html#getting-familiar",
    "href": "ASReviewLAB.html#getting-familiar",
    "title": "Introductory exercise to AI-aided screening using ASReview LAB",
    "section": "Getting familiar",
    "text": "Getting familiar\nBefore you start, you might want to read a bit more on:\n\nWhat is ASReview LAB?\nThe terminology used\nThe paper that was published in Nature Machine Intelligence"
  },
  {
    "objectID": "ASReviewLAB.html#the-software",
    "href": "ASReviewLAB.html#the-software",
    "title": "Introductory exercise to AI-aided screening using ASReview LAB",
    "section": "The software",
    "text": "The software\n\nStep 1: Installing Python and ASReview LAB\nFirst, you need to install Python.\nOnce you have Python installed, you can go through the easy 3-step guide to installing (or upgrading) ASReview on the ASReview-website.\nMore detailed installation information, troubleshooting, and installation on a server or via a Docker are available on ReadTheDocs.\nHave you installed the latest version of ASReview? You can proceed to step 2!\n\n\nStep 2: Starting ASReview LAB\nTo open ASReview LAB in your browser, you need to start it in the command prompt (i.e. CMD.exe or Terminal). You can open your command prompt by typing ‘cmd’ (for Windows) or ‘terminal’ (for Apple) in your computer’s search bar (select ‘Run as administrator’ if you have this option).\n\nThe command prompt will open, in which you can type the following command and press enter:\nasreview lab\n\nIt takes a few seconds for ELAS - your Electronic Learning Assistant - to start the software. It will appear in your (default) web browser.\nBut why do you need to start it up by running code in your command prompt? This ensures that ASReview LAB is run locally. More specifically, your data is and stays your own. Small price to pay for complete privacy, right?! Read more about the key principles in the Zen of Elas!\nNote that you have to keep your command-line interpreter running while using ASReview LAB, even though the interface is in your browser!\nYou can also run the software via a server, but you need to take care of hosting the server yourself (or ask your IT-department).\nHave you opened ASReview LAB in your browser? If so, you can proceed to step 3!\n\n\nStep 3: Creating a project\nNow that you have installed and opened ASReview LAB, you can create a new project. Below you will find a step-by-step guide. Note that the screenshots shown below are made in dark mode.\n\nNew project;\n\nHover your mouse over the ‘create’ button with the plus sign in the bottom right corner.\n\n\nProject name;\n\nSelect Exploration Mode, fill out a project name and press ‘NEXT’. Note that you can fill out your name and a description as well.\n\nFor this exercise we are screening in the so-called ‘Exploration Mode’ of ASReview. By screening in the Exploration Mode, we are going to make use of a benchmark dataset. This means that all records in the dataset have already been labeled as relevant or irrelevant. This is indicated to the user through a banner above each article. Note that in ‘Oracle Mode’ - when screening your own dataset - the relevant papers will not be marked; you, the oracle, have to make the decisions.\nMore detailed information about setting up a project can be found on ReadTheDocs.\nHave you started creating a new project? If so, you can proceed to step 4!"
  },
  {
    "objectID": "ASReviewLAB.html#project-setup",
    "href": "ASReviewLAB.html#project-setup",
    "title": "Introductory exercise to AI-aided screening using ASReview LAB",
    "section": "Project setup",
    "text": "Project setup\n\nStep 4: The dataset\nNow that you have created your ASReview project (woohoo!), you need to set it up.Without data, we have nothing to screen. So, you need to tell ELAS which dataset you want to screen for relevant articles.\nClick on the ‘ADD’ button next to ‘Add dataset’. Now a menu appears where you can choose how to load the dataset. You can add your dataset by selecting a file or providing an URL. For this exercise, we will use a benchmark dataset.\nGo to the ‘Benchmark datasets’ button, open the first dataset (i.e. the Van de Schoot (2017) dataset about PTSD trajectories) and click on ‘SELECT’. After you select the dataset, click on ‘SAVE’.\n\nHave you successfully selected/uploaded the dataset? If so, you can proceed to step 5!\n\n\nStep 5: Prior knowledge\nBefore you can start screening the records, you need to tell ELAS what kind of records you are and what kind of records you are not looking for (i.e., relevant and irrelevant records, respectively). We call this prior knowledge. Based on the prior knowledge you provide, ELAS will reorder the stack of papers and provide you with the record that is most likely to be relevant (default settings).\nWhen performing a systematic review with your own data, you need to provide the prior knowledge yourself (at least one relevant and one irrelevant record). However, because you are using the Exploration Mode of ASReview, the relevant records are known; the original authors have already read ALL records.\nTo select the prior knowledge you first need to click on the ‘ADD’ button next to ‘Add prior knowledge’; see also the documentation about the selection of prior knowledge. Now you will see a menu about selecting prior knowledge.\nThe following five papers are known to be relevant:\n\nLatent trajectories of trauma symptoms and resilience: the 3-year longitudinal prospective USPER study of Danish veterans deployed in Afghanistan (DOI: 10.4088/JCP.13m08914)\nA Latent Growth Mixture Modeling Approach to PTSD Symptoms in Rape Victims (DOI: 10.1177/1534765610395627)\nPeace and War: Trajectories of Posttraumatic Stress Disorder Symptoms Before, During, and After Military Deployment in Afghanistan (DOI: 10.1177/0956797612457389)\nThe relationship between course of PTSD symptoms in deployed U.S. Marines and degree of combat exposure (DOI: 10.1002/jts.21988)\nTrajectories of trauma symptoms and resilience in deployed US military service members: Prospective cohort study (DOI: 10.1192/bjp.bp.111.096552)\n\nTo add the relevant records, you click on ‘Search’, copy and paste the titles of these relevant records one by one in the search bar and add them as relevant.\n\nAfter adding all five relevant records, you can add some irrelevant ones by clicking the ‘Random’ button (use the arrow in the upper left corner to be able to select this button) and by changing ‘relevant’ to ‘irrelevant’. Select five irrelevant records and click on ‘CLOSE’.\nHave you selected five relevant and five irrelevant records? If so, you can proceed to step 6!\n\n\nStep 6: Active learning model\nThe last step to complete the setup is to select the active learning model you want to use. The default settings (i.e. Naïve Bayes, Max and tf-idf) will suffice for this exercise. If you want to change the mode, read which options are built-in or add your own model via a template.\nYou can click on ‘NEXT’. A menu with the defaults will appear. Since we are using the defaults, you can click on ‘NEXT’ again. In the last step of the setup, ASReview LAB runs the feature extractor, trains a model, and ranks the records in your dataset. Depending on the model and the size of your dataset, this can take a couple of minutes (meanwhile, you can enjoy the animation video or read the blog post on What is Active Learning?).\nAfter the project is successfully initialized, you can start reviewing.\n\nHave you finished the setup? If so, you can proceed to step 7!"
  },
  {
    "objectID": "ASReviewLAB.html#screening-phase",
    "href": "ASReviewLAB.html#screening-phase",
    "title": "Introductory exercise to AI-aided screening using ASReview LAB",
    "section": "Screening phase",
    "text": "Screening phase\n\nStep 7: Screening the records\nEverything is set up and ready to screen, well done!\nSince we are in the Exploration Mode of ASReview, you can pretend to be an expert on the topic of PTSD and pretend you have all the knowledge of the original screeners. All records in the dataset have been labeled as relevant/irrelevant, which is indicated through a banner above each article. Click on the heart shaped Relevant button if the record is marked as relevant. If not, you can press the Irrelevant button.\nNow, all we need is a Stopping Rule to determine when you are confident that you have identified (almost) all relevant records in your dataset. For this exercise, continue screening records until you have marked 50 consecutive records as irrelevant. You can check up on your progress in the Analytics page.\nWhen you have reached your Stopping Rule and you are done screening, go back to the Analytics page. Here you can see the summary statistics of your project such as the number of records you have labeled relevant or irrelevant. It also shows how many records are in your dataset and how many records you labeled irrelevant since you have screened the last relevant record. For more information about how to read these summary statistics and interpret the corresponding charts, check out the documentation on the Analytics page.\nThe Van de Schoot (2017) dataset contained 43 relevant records in this particular example. Did you get to label all of them as relevant before you reached your Stopping Rule? If you did, great!\nWhat is the percentage of total papers you needed to screen to find the number of relevant records you have found? Is it &lt;100%? Then, you were quicker compared to the original screeners of the dataset!\nYou probably had to screen about only 2-3% of the data. Amazing right?! Chances are though that you did not get to see a couple of relevant records before you stopped screening. Do you think this is acceptable? There is a trade-off between the time spent screening and the error rate: the more records you screen, the lower the risk of missing a relevant record. However, screening all records in your dataset is still no guarantee for an error rate of zero, since even traditional screening by humans - which is the gold standard to which we compare AI-assisted screening - is not perfect 1.\nYour willingness to accept the risk that you may exclude some relevant records is something to take into account when deciding on a Stopping Rule. Read more about Stopping Rules and how to decide on a good strategy for your data on the discussion platform.\n\n\nStep 8: Extracting and inspecting the data\nNow that you found all or most relevant records, you can export your data using these instructions. If you choose to inspect your data in Excel, download the data in ‘Excel’ format. If you prefer to inspect your data in R, download the ‘CSV (UTF-8)’ format and open it in R.\nYou can find all the data that was originally imported to ASReview in the exported data file, in a new order and with two new columns added at the end.\nUsing the information about the Read the Docs page can you reorder your data to appear in the order in which you loaded them into ASReview? And back to the order provided by ASReview?\nCheck if the number of records coded included = 1 corresponds to the number of relevant records on your Analytics page. From which row number, based on the original ordering, do the included articles come from?\nFor the last exercise, it is important to change the order back to the order provided by ASReview. Lastly, check out the first few records with no number in the included column. Are those articles labeled as ‘relevant’ in the original dataset? (Whether or not a record was pre-labeled as relevant is shown in the column label_included in the original dataset.)"
  },
  {
    "objectID": "ASReviewLAB.html#goal",
    "href": "ASReviewLAB.html#goal",
    "title": "Introductory exercise to AI-aided screening using ASReview LAB",
    "section": "Goal",
    "text": "Goal\nIn the beginning of the LAB the following goal was specified: “The goal of this LAB is to get familiar with AI-aided screening by making use of ASReview LAB.” Did you achieve this goal?\nIf so: congratulations! You now know all the steps to create and screen for a systematic review. ELAS wishes you a lot of fun screening with ASReview!\nDo you like the software, leave a star on Github; this will help to increase the visibility of the open-source project."
  },
  {
    "objectID": "ASReviewLAB.html#whats-next",
    "href": "ASReviewLAB.html#whats-next",
    "title": "Introductory exercise to AI-aided screening using ASReview LAB",
    "section": "What’s next?",
    "text": "What’s next?\nSome suggestions:\n\nRead a blog posts about:\n\nFive ways to get involved in ASReview,\nSeven ways to integrate ASReview in your systematic review workflow,\nActive learning explained.\n\nReady to start your own project? Upload your own data and start screening in the Oracle mode!\nTry to find the hidden memory game in ASReview (some people found it by going through the source code on Github… +1 for open-science!)"
  },
  {
    "objectID": "datatools.html",
    "href": "datatools.html",
    "title": "Introductory exercise on Using ASReview Datatools",
    "section": "",
    "text": "The goal of this exercise is to learn how to use ASReview Datatools for processing a dataset.\nWhen performing a systematic review, it is vital to proparly prepare your dataset. A dataset needs to contain at least a ‘title’ and ‘abstract’ column. Another column that comes in handy is for example ‘doi’. After you created such a dataset, it can be the case that some preprocessing or - after screening the literature - postprocessing is needed. It can for example be that the dataset contains a lot of duplicates.\nIn this exercise, you will learn how to describe, compose and convert datasets. If writing scripts is new to you, don’t worry, you will be guided through the process in the command-line interface.\nEnjoy!\n\n\nWe recommend you to create a designated folder to run a simulation in, to keep your project organised. Create a folder and give it a name, for example “Data_preprocessing_PTSD”. Within this folder, create two sub-folders called ‘data’ and ‘output’. Also using NotePad (on Windows) or Text Edit (on Mac/OS) to create an empty text file called jobs.txt in the main folder. This is where you will save the scripts you run.\n\n\n\nBefore we get started, we need a dataset that is already labeled. You can use a dataset from the SYNERGY dataset via the synergy-dataset Python package.\n\nOpen the command prompt, by typing ‘cmd’ (for Windows) or ‘terminal’ (for Apple) in your computer’s search bar;\nNavigate to the folder structure;\n\nYou can navigate to your folder structure with:\ncd [file_path]\n\nInstall the synergy-dataset Python package with:\n\npip install synergy-dataset\n\nBuilt the dataset.\n\nTo download and build the SYNERGY dataset, run the following command in the command line:\npython -m synergy_dataset get -o data\nYou can also choose to use your own dataset(s). For the purpose of this exercise, do make sure that the dataset consists of at least a ‘title’ and ‘abstract’ and ‘doi’ column.\n\n\n\nFor the next steps, you first need to install ASReview datatools. You can do so with:\npip install asreview-datatools\nNote: if you installed ASReview datatools before, make sure you install the latest version. You can do so with:\npip install --upgrade asreview-datatools\nBefore you are going to write your script, here is a general explaination of how you can get started with ASReview datatools. The parts between the brackets need to be filled out by you when using ASReview Datatools.\nasreview data [name_of_tool]\nwhere [name_of_tool] is the name of the tool you want to use (describe, convert, dedup, vstack, or compose) followed by positional arguments and optional arguments.\nEach tool has its own help description which is available with:\nasreview data [name_of_tool] -h\n\n\n\nWith the describe tool you get information about the properties of your dataset. For example, it shows you how many total, relevant, irrelevant and unlabeled records your dataset contains.\nYou can describe the content of a dataset with:\nasreview data describe data/[name_of_your_data.csv]\nNote: ‘data/’ is in front of the name of your dataset, since the datasets are stored in the ‘data’ folder you created in Step 1 and not in the directory you are currently working from.\n\n\n\nNext, you are going to compose a dataset out of two datasets; a labeled and an unlabeled dataset. The code starts similary to the code of the Describe tool (asreview data compose), but now the output path should always be specified (output/[name_of_your_output_file.csv]) and you need to assign a corresponsing proporty to the datasets you want to merge. The labeled dataset you need to assign the argument -l and the unlabeled dataset you need to assign a -u.\nFurthermore, in case of conflict, you need to determine which label to keep (-c). Let’s say you want to keep one of the labels, you can do so with the keep_one command. By default the hierarchy in determining which label to keep is: relevant, irrelevant, unlabeled.\nSee the Datatools README for other possible arguments.\nYou can compose a dataset with:\nasreview data compose output/[name_of_your_output_file.csv] -l data/[name_of_your_data.csv] -u data/[name_of_your_data.csv] -c keep_one\n\n\n\nNow you are going to convert the format of your dataset. You are going to convert a RIS dataset into a CSV dataset.\nFind a RIS file on you laptop, put it in the data folder you created in Step 1 and try the following command:\nasreview data convert data/[name_of_your_data].ris output/[name_of_your_output_file].csv\nIf you don’t have a RIS file, you can practice with the code by converting a CSV file from the Synergy dataset into an Excel file:\nasreview data convert data/[name_of_your_data].csv output/[name_of_your_output_file].xlsx"
  },
  {
    "objectID": "datatools.html#introduction",
    "href": "datatools.html#introduction",
    "title": "Introductory exercise on Using ASReview Datatools",
    "section": "",
    "text": "The goal of this exercise is to learn how to use ASReview Datatools for processing a dataset.\nWhen performing a systematic review, it is vital to proparly prepare your dataset. A dataset needs to contain at least a ‘title’ and ‘abstract’ column. Another column that comes in handy is for example ‘doi’. After you created such a dataset, it can be the case that some preprocessing or - after screening the literature - postprocessing is needed. It can for example be that the dataset contains a lot of duplicates.\nIn this exercise, you will learn how to describe, compose and convert datasets. If writing scripts is new to you, don’t worry, you will be guided through the process in the command-line interface.\nEnjoy!\n\n\nWe recommend you to create a designated folder to run a simulation in, to keep your project organised. Create a folder and give it a name, for example “Data_preprocessing_PTSD”. Within this folder, create two sub-folders called ‘data’ and ‘output’. Also using NotePad (on Windows) or Text Edit (on Mac/OS) to create an empty text file called jobs.txt in the main folder. This is where you will save the scripts you run.\n\n\n\nBefore we get started, we need a dataset that is already labeled. You can use a dataset from the SYNERGY dataset via the synergy-dataset Python package.\n\nOpen the command prompt, by typing ‘cmd’ (for Windows) or ‘terminal’ (for Apple) in your computer’s search bar;\nNavigate to the folder structure;\n\nYou can navigate to your folder structure with:\ncd [file_path]\n\nInstall the synergy-dataset Python package with:\n\npip install synergy-dataset\n\nBuilt the dataset.\n\nTo download and build the SYNERGY dataset, run the following command in the command line:\npython -m synergy_dataset get -o data\nYou can also choose to use your own dataset(s). For the purpose of this exercise, do make sure that the dataset consists of at least a ‘title’ and ‘abstract’ and ‘doi’ column.\n\n\n\nFor the next steps, you first need to install ASReview datatools. You can do so with:\npip install asreview-datatools\nNote: if you installed ASReview datatools before, make sure you install the latest version. You can do so with:\npip install --upgrade asreview-datatools\nBefore you are going to write your script, here is a general explaination of how you can get started with ASReview datatools. The parts between the brackets need to be filled out by you when using ASReview Datatools.\nasreview data [name_of_tool]\nwhere [name_of_tool] is the name of the tool you want to use (describe, convert, dedup, vstack, or compose) followed by positional arguments and optional arguments.\nEach tool has its own help description which is available with:\nasreview data [name_of_tool] -h\n\n\n\nWith the describe tool you get information about the properties of your dataset. For example, it shows you how many total, relevant, irrelevant and unlabeled records your dataset contains.\nYou can describe the content of a dataset with:\nasreview data describe data/[name_of_your_data.csv]\nNote: ‘data/’ is in front of the name of your dataset, since the datasets are stored in the ‘data’ folder you created in Step 1 and not in the directory you are currently working from.\n\n\n\nNext, you are going to compose a dataset out of two datasets; a labeled and an unlabeled dataset. The code starts similary to the code of the Describe tool (asreview data compose), but now the output path should always be specified (output/[name_of_your_output_file.csv]) and you need to assign a corresponsing proporty to the datasets you want to merge. The labeled dataset you need to assign the argument -l and the unlabeled dataset you need to assign a -u.\nFurthermore, in case of conflict, you need to determine which label to keep (-c). Let’s say you want to keep one of the labels, you can do so with the keep_one command. By default the hierarchy in determining which label to keep is: relevant, irrelevant, unlabeled.\nSee the Datatools README for other possible arguments.\nYou can compose a dataset with:\nasreview data compose output/[name_of_your_output_file.csv] -l data/[name_of_your_data.csv] -u data/[name_of_your_data.csv] -c keep_one\n\n\n\nNow you are going to convert the format of your dataset. You are going to convert a RIS dataset into a CSV dataset.\nFind a RIS file on you laptop, put it in the data folder you created in Step 1 and try the following command:\nasreview data convert data/[name_of_your_data].ris output/[name_of_your_output_file].csv\nIf you don’t have a RIS file, you can practice with the code by converting a CSV file from the Synergy dataset into an Excel file:\nasreview data convert data/[name_of_your_data].csv output/[name_of_your_output_file].xlsx"
  },
  {
    "objectID": "datatools.html#what-else",
    "href": "datatools.html#what-else",
    "title": "Introductory exercise on Using ASReview Datatools",
    "section": "What else?",
    "text": "What else?\nWe introduced some of the datatools, but there are more tools available in the ASReview Datatools package.\nIf you like the functionality of Datatools, don’t forget to give it a star on GitHub!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ASReview Acadamy",
    "section": "",
    "text": "Welcome to the ASReview Academy! This website contains open teaching materials for using ASReview. These resources are created to facilitate learning and proficiency in utilizing the ASReview software for AI-aided systematic reviews. We hope you enjoy the materials and find them useful."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "ASReview Acadamy",
    "section": "License",
    "text": "License\nThe work is published under the CC BY 4.0 license."
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "Asreview Acadamy",
    "section": "",
    "text": "Attribution 4.0 International\n=======================================================================\nCreative Commons Corporation (“Creative Commons”) is not a law firm and does not provide legal services or legal advice. Distribution of Creative Commons public licenses does not create a lawyer-client or other relationship. Creative Commons makes its licenses and related information available on an “as-is” basis. Creative Commons gives no warranties regarding its licenses, any material licensed under their terms and conditions, or any related information. Creative Commons disclaims all liability for damages resulting from their use to the fullest extent possible.\nUsing Creative Commons Public Licenses\nCreative Commons public licenses provide a standard set of terms and conditions that creators and other rights holders may use to share original works of authorship and other material subject to copyright and certain other rights specified in the public license below. The following considerations are for informational purposes only, are not exhaustive, and do not form part of our licenses.\n Considerations for licensors: Our public licenses are\n intended for use by those authorized to give the public\n permission to use material in ways otherwise restricted by\n copyright and certain other rights. Our licenses are\n irrevocable. Licensors should read and understand the terms\n and conditions of the license they choose before applying it.\n Licensors should also secure all rights necessary before\n applying our licenses so that the public can reuse the\n material as expected. Licensors should clearly mark any\n material not subject to the license. This includes other CC-\n licensed material, or material used under an exception or\n limitation to copyright. More considerations for licensors:\nwiki.creativecommons.org/Considerations_for_licensors\n\n Considerations for the public: By using one of our public\n licenses, a licensor grants the public permission to use the\n licensed material under specified terms and conditions. If\n the licensor's permission is not necessary for any reason--for\n example, because of any applicable exception or limitation to\n copyright--then that use is not regulated by the license. Our\n licenses grant only permissions under copyright and certain\n other rights that a licensor has authority to grant. Use of\n the licensed material may still be restricted for other\n reasons, including because others have copyright or other\n rights in the material. A licensor may make special requests,\n such as asking that all changes be marked or described.\n Although not required by our licenses, you are encouraged to\n respect those requests where reasonable. More_considerations\n for the public:\nwiki.creativecommons.org/Considerations_for_licensees\n=======================================================================\nCreative Commons Attribution 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\nSection 1 – Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter’s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\nSection 2 – Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\n\nreproduce and Share the Licensed Material, in whole or in part; and\nproduce, reproduce, and Share Adapted Material.\n\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)\n\nnever produces Adapted Material.\n\nDownstream recipients.\n\nOffer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nNo downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\n\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\n\nSection 3 – License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\n\nretain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nindicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nindicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\n\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\nIf You Share Adapted Material You produce, the Adapter’s License You apply must not prevent recipients of the Adapted Material from complying with this Public License.\n\n\nSection 4 – Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material; and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\nSection 5 – Disclaimer of Warranties and Limitation of Liability.\n\nUNLESS OTHERWISE SEPARATELY UNDERTAKEN BY THE LICENSOR, TO THE EXTENT POSSIBLE, THE LICENSOR OFFERS THE LICENSED MATERIAL AS-IS AND AS-AVAILABLE, AND MAKES NO REPRESENTATIONS OR WARRANTIES OF ANY KIND CONCERNING THE LICENSED MATERIAL, WHETHER EXPRESS, IMPLIED, STATUTORY, OR OTHER. THIS INCLUDES, WITHOUT LIMITATION, WARRANTIES OF TITLE, MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, NON-INFRINGEMENT, ABSENCE OF LATENT OR OTHER DEFECTS, ACCURACY, OR THE PRESENCE OR ABSENCE OF ERRORS, WHETHER OR NOT KNOWN OR DISCOVERABLE. WHERE DISCLAIMERS OF WARRANTIES ARE NOT ALLOWED IN FULL OR IN PART, THIS DISCLAIMER MAY NOT APPLY TO YOU.\nTO THE EXTENT POSSIBLE, IN NO EVENT WILL THE LICENSOR BE LIABLE TO YOU ON ANY LEGAL THEORY (INCLUDING, WITHOUT LIMITATION, NEGLIGENCE) OR OTHERWISE FOR ANY DIRECT, SPECIAL, INDIRECT, INCIDENTAL, CONSEQUENTIAL, PUNITIVE, EXEMPLARY, OR OTHER LOSSES, COSTS, EXPENSES, OR DAMAGES ARISING OUT OF THIS PUBLIC LICENSE OR USE OF THE LICENSED MATERIAL, EVEN IF THE LICENSOR HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH LOSSES, COSTS, EXPENSES, OR DAMAGES. WHERE A LIMITATION OF LIABILITY IS NOT ALLOWED IN FULL OR IN PART, THIS LIMITATION MAY NOT APPLY TO YOU.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\nSection 6 – Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\nSection 7 – Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\nSection 8 – Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n=======================================================================\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” The text of the Creative Commons public licenses is dedicated to the public domain under the CC0 Public Domain Dedication. Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org."
  },
  {
    "objectID": "simulation.html",
    "href": "simulation.html",
    "title": "Introductory exercise on Using the ASReview Simulation Mode",
    "section": "",
    "text": "The goal of this exercise is to learn how you can perform simulations on a labeled systematic review dataset using the web browser of ASReview, the command line interface, and the package titled Makita.\nA simulation in the context of AI-assisted reviewing involves mimicking the screening process with a particular model. In a simulation setting, it is already known which records are labeled as relevant, and the software can automatically reenact the screening process as if a human were labeling the records in interaction with the Active Learning model. This allows you to answer questions about the (workings of) the model itself and compare the performance of different models under different circumstances. Read the documentation for more information about running simulations.\nIf writing scripts and running simulations is new to you, don’t worry, you can make it as simple or complicated as you want.\nIn this exercise, you will learn how to prepare, run, and interpret the results of a simulation yourself. You will first do this using the web browser with the click-and-run interface you know from the first exercise. Then you will be guided through the same process in the command-line interface, which has more options than the LAB interface. Lastly, you will run the same simulations, this time using the Makita package. This package automates much of the process, which is great for large-scale simulation studies.\nEnjoy!\n\n\n\nExample of what the output of multiple simulation studies can look like. This shows the recall curves for the first 4K records (out of &gt;50K) for 10 simulation runs, each with a different model configuration, so you can compare their performance on any given dataset at a glance. Source: Teijema, J., Hofstee, L., Brouwer, M., de Bruin, J., Ferdinands, G., de Boer, J., … Bagheri, A. (2022, July 15). Active learning-based Systematic reviewing using switching classification models: the case of the onset, maintenance, and relapse of depressive disorders. https://doi.org/10.31234/osf.io/t7bpd”"
  },
  {
    "objectID": "simulation.html#introduction",
    "href": "simulation.html#introduction",
    "title": "Introductory exercise on Using the ASReview Simulation Mode",
    "section": "",
    "text": "The goal of this exercise is to learn how you can perform simulations on a labeled systematic review dataset using the web browser of ASReview, the command line interface, and the package titled Makita.\nA simulation in the context of AI-assisted reviewing involves mimicking the screening process with a particular model. In a simulation setting, it is already known which records are labeled as relevant, and the software can automatically reenact the screening process as if a human were labeling the records in interaction with the Active Learning model. This allows you to answer questions about the (workings of) the model itself and compare the performance of different models under different circumstances. Read the documentation for more information about running simulations.\nIf writing scripts and running simulations is new to you, don’t worry, you can make it as simple or complicated as you want.\nIn this exercise, you will learn how to prepare, run, and interpret the results of a simulation yourself. You will first do this using the web browser with the click-and-run interface you know from the first exercise. Then you will be guided through the same process in the command-line interface, which has more options than the LAB interface. Lastly, you will run the same simulations, this time using the Makita package. This package automates much of the process, which is great for large-scale simulation studies.\nEnjoy!\n\n\n\nExample of what the output of multiple simulation studies can look like. This shows the recall curves for the first 4K records (out of &gt;50K) for 10 simulation runs, each with a different model configuration, so you can compare their performance on any given dataset at a glance. Source: Teijema, J., Hofstee, L., Brouwer, M., de Bruin, J., Ferdinands, G., de Boer, J., … Bagheri, A. (2022, July 15). Active learning-based Systematic reviewing using switching classification models: the case of the onset, maintenance, and relapse of depressive disorders. https://doi.org/10.31234/osf.io/t7bpd”"
  },
  {
    "objectID": "simulation.html#conducting-a-simulation-via-the-web-browser",
    "href": "simulation.html#conducting-a-simulation-via-the-web-browser",
    "title": "Introductory exercise on Using the ASReview Simulation Mode",
    "section": "Conducting a simulation via the web browser",
    "text": "Conducting a simulation via the web browser\nTo start off easy, we will run a simulation study using the PTSD benchmark dataset we also used in the introductory exercise.\nFor this exercise, we assume you have already installed Python and the ASReview LAB application. If you haven’t done so yet, you can follow the instructions on installation.\n\nOpen ASReview LAB via the CLI (Command Line Interface) and start a new project in Simulation Mode. Follow the steps described in the documentation and select the PTSD benchmark dataset.\n\nAdd these five prior relevant studies:\n\nLatent trajectories of trauma symptoms and resilience: the 3-year longitudinal prospective USPER study of Danish veterans deployed in Afghanistan\nA Latent Growth Mixture Modeling Approach to PTSD Symptoms in Rape Victims\nPeace and War: Trajectories of Posttraumatic Stress Disorder Symptoms Before, During, and After Military Deployment in Afghanistan\nThe relationship between course of PTSD symptoms in deployed U.S. Marines and degree of combat exposure\nTrajectories of trauma symptoms and resilience in deployed US military service members: Prospective cohort study\n\nNow, also choose five (random) prior irrelevant papers and copy-paste these titles to a separate file, as we need to select the exact same studies later!\nSelect the default model and initiate the simulation by clicking “NEXT”.\nProceed by navigating to the Analytics page and check if the simulation study is finished. ELAS is holding a card with the status of your project. If your simulation is not finished yet, click the REFRESH button in the upper right corner.\nNote: it can be that running the simulation takes a long time because of the size of your dataset in combination with the chosen model. To give you an idea of the possible differences in runtime between models, see Table 1 on page 15 of Teijema et al. (2022). Training the feature matrix for a dataset with 15K records took .23 seconds with TF-IDF, but 6 hours with sBert.\nWhen the simulation is done, save the recall plot on your laptop. For instructions on how to do this, see the documentation.\nWoohoo! You just performed a simulation study! Easy right?\nLet’s do some more:\n\nStart a new project (use a different name), add the same dataset, and add the same relevant and irrelevant articles as priors but this time choose a different classification model such as Random Forest or Logistic Regression. Run the simuation study and save the recall plot.\nRepeat the same process, but now choose a different feature extractor, query strategy or balancing strategy. Run the simulation and save the recall plot.\n\nCompare the three recall plots. What is your conclusion in terms of performance for the three different models? You might observe some differences in how long it takes each model to arrive at the total number of relevant records found.\nMany papers have been written about such comparisons; for an overview of such comparisons, see the systematic review on this topic."
  },
  {
    "objectID": "simulation.html#conducting-a-simulation-via-the-command-line-interface",
    "href": "simulation.html#conducting-a-simulation-via-the-command-line-interface",
    "title": "Introductory exercise on Using the ASReview Simulation Mode",
    "section": "Conducting a simulation via the command line interface",
    "text": "Conducting a simulation via the command line interface\n\nStep 1: Choose a research question\nMany more options for the configuration are available via the command line interface. This is useful because 1001 questions can be answered by running a simulation study and every simulation study requires slightly different settings. As example, we will work on three example questions.\nIn the second part of the exercise, you will also be introduced to some performance metrics that you can use to assess the performance of your model.\nResearch questions:\n\n\n\nWhen using AI-aided screening to find 95 percent of the relevant records, how much time did I save compared to random screening?\nThis question can be addressed by examining the Work Saved over Sampling (WSS) statistic at a recall of .95, which reflects the proportion of screening time saved by using active learning at the cost of failing to identify .05 of relevant publications.\n\n\n\nHow much variation is there between three runs of the same model with different prior records selected for training the model?\nThis question can be addressed by running the exact same simulation three times, but with different prior knowledge selection. We introduce the metric the percentage of Relevant Records Found (RRF) at a given recall (e.g., after screening 10% of the total number of records).\n\n\n\nDoes, for example, the classifier Naive Bayes (‘nb’) or Logistic Regression (‘logistic’) perform better for a given dataset, when keeping the other settings fixed? For this question, we introduce the metrics the Extra Relevant Records found (ERF) and the Average Time to Discovery (ATD).\nTo keep your results organized, we suggest you fill out the table below with your simulation results for each model you run.\n\n\nStep 2: Create a folder structure\nWe recommend you to create a designated folder to run a simulation in, to keep your project organised. Create a folder and give it a name, for example “Simulation_study_PTSD”. Within this folder, create two sub-folders called ‘data’ and ‘output’. Also using NotePad (on Windows) or Text Edit (on Mac/OS) to create an empty text file called jobs.txt in the main folder. This is where you will save the scripts you run. See the example below:\n\n\n\nStep 3: Choose a dataset\nBefore we get started, we need a dataset that is already labeled. You can use a dataset from the SYNERGY dataset via the synergy-dataset Python package.\n\nOpen the command prompt, by typing ‘cmd’ (for Windows) or ‘terminal’ (for Apple) in your computer’s search bar;\nNavigate to the folder structure;\n\nYou can navigate to your folder structure with:\ncd [file_path]\n\nInstall the synergy-dataset Python package with:\n\npip install synergy-dataset\n\nBuilt the dataset.\n\nTo download and build the SYNERGY dataset, run the following command in the command line:\npython -m synergy_dataset get -o data\nYou can also choose to use your own dataset. Do make sure that the dataset is fully labeled and prepared for simulation.\n\n\nStep 4: Write and run the simulation script\nBelow you will find a step by step guide on how to run a simulation with the ASReview software. Depending on your research question, you need to add or change things (e.g.run multiple simulations if you want to compare runs of the same model with different seeds or to compare different models question 2 and 3).\nThe parts between the brackets [ ] need to be filled out by you.\n\nOpen the command prompt, by typing ‘cmd’ (for Windows) or ‘terminal’ (for Apple) in your computer’s search bar;\nNavigate to the folder structure;\n\nYou can navigate to your folder structure with:\ncd [file_path]\nThe file_path is the path to the folder which you can copy and paste from your explorer.\n\nRun the simulation;\n\nNow, we’re going to write the script to run your simulation, using the code below. You still need to fill out at least three pieces of information: the dataset’s name, the name of the output file and two seed values, to make the results reproducible. Some processes in ASReview require (pseudo-)random number generation. The user can give such a process a fixed “seed” to start from, which means the same sequence is generated each time, making any simulation reproducible. The --model_seed command controls the seed of the random number generation that is used after model initialization. The --init_seed command controls the random set of papers to train the model on.\nasreview simulate data/[name_of_your_data.csv] --state_file output/[results_name].asreview \n--seed [your favorite number] --init_seed [your second favorite number]\nSave this script in the jobs.txt file you just made, to keep track of which code you ran!\nYou can also decide to supply more arguments, for example to employ a different model or to specify prior knowledge yourself instead of using --init_seed to select random ones . If no prior knowledge is specified, one relevant and one irrelevant prior are randomly selected. If no model is specified, the software’s default active learning model is used.\nRun the simulation by copy-pasting the code in the command line interface.\nWhile your simulation is running, you can check whether the priors were correctly specified as they appear in the command prompt and you can stare at the progress bar.\n\n\nStep 5: Obtaining a recall curve and the metrics\nFor the next steps, you first need to install some extra extensions. You can do so with:\npip install asreview-insights asreview-datatools asreview-wordcloud asreview-makita\nYou can get the recall plot with this line of code:\nasreview plot recall output/[results_name].asreview -o output/results.png\nRun the code in the CLI (and put it in your jobs.txt file).\nThe plot file will appear in your output folder. Inspect the recall curve from the asreview-insights extension. It should look something like the figure below.\n\nYou can get the metrics with:\nasreview metrics output/[results_name].asreview -o output/output.json\nThe output will appear in your command prompt and will be saved in the ‘output.json’ file in your output folder. You can scroll up in your CLI or open the file with a text editor like Notepad (or R, Python, or a Jupiter Notebook) to look at the metrics we got from the asreview-insights.\nFill out the performance statistics in your table under model 1. But what do those statistics mean? Check out the readme of the Insights packages.\n\n\nResearch Question 1\nTo answer research question 1, look at the WSS@95 statistic, reflecting the proportion of records you did not have to screen by using active learning compared to random reading, at the cost of failing to identify .05 of relevant publications. In the above example, we did not have to screen 65% of the records to find 95% of relevant records in the dataset.\n\n\nResearch Question 2\nTo answer research question 2, you can rerun the same model once more with a different value for --init_seed. A different seed value will result in a different selection of prior knowledge.\nMake sure to store the results under a different name and write the results in your table under Model 2.\nCan you confirm that the model used different prior knowledge in this run using the information printed in your console?\nThen, run the model a third time (Model 3), but this time specify a specific set of records to be used as training data by using the --prior_idx argument:\nasreview simulate data/[name_of_your_data.csv] --state_file output/[results_name].asreview \n--seed [your favorite number] --prior_idx [row nr. relevant record] [row nr. irrelevant record]\nYou should search in the data to identify the row numbers of the records you want to use. Be aware we start counting row numbers at zero (and mind the column names).\nYou can compare the three runs by looking at (for example) the variation in recall curves and the RRF@10 statistics. A higher percentage of Relevant Records Found after screening any portion of the total records (in this case 10%), indicates a more efficient model.\n\n\nResearch Question 3\nTo answer research question 3, rerun the model with the logistic model classifier instead of the default Naive Bayes (nb) setting:\nasreview simulate data/[name_of_your_data].csv -m logistic \n--state_file output/[results_name].asreview  --seed [your favorite number]\n--init_seed [your second favorite number]\nYou can compare the performance of the models by looking at the (for example) ERF and the ATD. A higher number of Extra Relevant Records found compared to random screening, indicates a a more efficient model. Conversely, a lower Average Time to Discovery means that the model was quicker to find all relevant records in the dataset."
  },
  {
    "objectID": "simulation.html#make-it-automatic",
    "href": "simulation.html#make-it-automatic",
    "title": "Introductory exercise on Using the ASReview Simulation Mode",
    "section": "Make it automatic",
    "text": "Make it automatic\nBy now, you can probably imagine that writing the code for running many different simulations is quite time-consuming. Keeping track of everything you ran in a separate jobs.txt file to keep your simulations reproducible and manually filling out that table for comparison can also become cumbersome for large projects. Luckily we have automatized the process ofcreating a folder structure and writing many lines of code by using templates. For this, we use the package Makita: ASReview’s Make It Automatic.\nWith a single command, Makita generates a folder according to a reproducible folder format, a jobs script with all simulation commands and even a readme file! This way, your study is ready for the world right off the bat.\nFor the next steps, you first need to install some extra extensions. You can do so with:\npip install asreview-insights asreview-datatools asreview-wordcloud asreview-makita\n\nGetting started\n\nFollow the first three steps under the Getting     started section. The result should be a new project folder including a subfolder titled ‘data’. Put at least two benchmark datasets in the ‘data’ folder. Make sure to save them as .csv files!\nNote: if multiple datasets are available in this folder, Makita will automatically create code for running a simulation study for all datasets stored in the data-folder.\nTo create a study using the basic template, navigate to the folder and run the code:\nasreview makita template basic\nIf you’re on a Windows machine, make sure to read the information in the Windows support section. In Windows, your final command should look like this:\nasreview makita template basic -f jobs.bat\nRun the generated jobs file by clicking on it for Windows, or by typing jobs.sh (Mac/OS) in the CLI. This will start the simulation.\n\nAfter the simulations have been completed, explore the README.md file that has been created in your project folder. This file can be opened with a text editor and contains a navigation tree showing which output is stored where.\nA big advantage of using Makita is that your project folder is already made fully reproducible and ready for publishing on your Github! Also note that in contrast with the code you ran above, we did not ask you to set a seed. That is because Makita takes care of this too! Makita sets a default seed for you, which automatically makes your simulation study reproducible. If you prefer to set the seed yourself, for example to avoid long-term seed bias, you can still do so using the optional arguments.\n\n\nAddressing the research questions\nNow, we are going to address the research question(s), this time using Makita.\n\n\n\nFor this question, compare the WSS@95 statistic of multiple datasets. If you had not done so already in the above section, pick two or three datasets from the benchmark datasets and save them to the ‘data’ folder. Use the basic template to run your simulations and navigate to the output folder. You can find the WSS@95 of the simulation for each of your datasets in the ‘output/tables’ folder, in the file data_metrics. Compare the WSS@95 statistics, for which of the datasets the metric is lowest?\n\n\n\nSuppose we are investigating the effect using different prior knowledge of relevant records, i.e. keeping the prior knowledge of irrelevant records stable. To answer this question, we can conveniently use the All Relevant, Fixed Irrelevant (ARFI) template. You only need one dataset in your ‘data’ folder, but you can choose to use multiple if you like.\nRun the following code if you work on Mac/OS:\nasreview makita template arfi\nor this line if you work on Windows\nasreview makita template arfi -f jobs.bat\nThen run the jobs.sh/bat file.\nNB: One simulation will be run for each relevant record in the dataset (with 50 relevant records, 50 simulation runs will be conduct; for each run the same 10 randomly chosen irrelevant records will be used), so this can take a while! Keep your CLI open to keep an eye on the progress, or go drink some hot chocolate.\nThe metrics can be found appear in the ‘tables’ folder. A recall plot summarizing all results can be found in the folder output/simulation/[DATASET NAME].\n\n\n\nMakita also contains a template to easily compare multiple models.\nRun the following code if you work on Mac/OS:\nasreview makita template multiple_models\nor this line if you work on Windows\nasreview makita template multiple_models -f jobs.bat\nThen run the jobs.sh/bat file.\nThe metrics can be found appear in the ‘tables’ folder. A recall plot summarizing all results can be found in the folder output/simulation/[DATASET NAME]."
  },
  {
    "objectID": "simulation.html#what-else",
    "href": "simulation.html#what-else",
    "title": "Introductory exercise on Using the ASReview Simulation Mode",
    "section": "What else?",
    "text": "What else?\nWe introduced some basic templates, but you can answer many more different types of research questions using Makita by using customized templates, or adding new models.\nIf you like the functionality of Makita, don’t forget to give it a star on GitHub!"
  }
]